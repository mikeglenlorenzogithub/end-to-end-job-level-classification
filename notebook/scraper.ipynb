{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dc34be",
   "metadata": {},
   "source": [
    "# Google Jobs Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414b874",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_export_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06647400",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "website = ENV['SOURCE']['NAME']\n",
    "version = ENV['VERSION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361a97a",
   "metadata": {},
   "source": [
    "## Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job(\n",
    "        scrape_date: datetime,\n",
    "        website: str,\n",
    "        version: int,\n",
    "        session: requests,\n",
    "        page: int,\n",
    "        max_retry: int=3,\n",
    "        time_sleep_min: int=1,\n",
    "        time_sleep_max: int=3,\n",
    "        timeout: int=10\n",
    "    ):\n",
    "\n",
    "    headers = {\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'accept-language': 'en-US,en;q=0.9,id;q=0.8',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'downlink': '1.35',\n",
    "        'priority': 'u=0, i',\n",
    "        'rtt': '300',\n",
    "        'sec-ch-prefers-color-scheme': 'dark',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"142\", \"Google Chrome\";v=\"142\", \"Not_A Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-arch': '\"x86\"',\n",
    "        'sec-ch-ua-bitness': '\"64\"',\n",
    "        'sec-ch-ua-form-factors': '\"Desktop\"',\n",
    "        'sec-ch-ua-full-version': '\"142.0.7444.60\"',\n",
    "        'sec-ch-ua-full-version-list': '\"Chromium\";v=\"142.0.7444.60\", \"Google Chrome\";v=\"142.0.7444.60\", \"Not_A Brand\";v=\"99.0.0.0\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-model': '\"\"',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'sec-ch-ua-platform-version': '\"19.0.0\"',\n",
    "        'sec-ch-ua-wow64': '?0',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "        'x-browser-channel': 'stable',\n",
    "    }\n",
    "\n",
    "    url_scrape = f'https://www.google.com/about/careers/applications/jobs/results?page={page}'\n",
    "\n",
    "    print(f\"Scraping Page: {page} | URL: {url_scrape}\")\n",
    "\n",
    "    for retry in range(max_retry):\n",
    "        time.sleep(random.uniform(time_sleep_min, time_sleep_max))\n",
    "        try:\n",
    "            response = session.get(\n",
    "                url_scrape,\n",
    "                headers=headers,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            # Validate Status Code\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError(f\"Status Code: {response.status_code} | URL: {url_scrape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if retry == max(range(max_retry)):\n",
    "                raise e\n",
    "            else:\n",
    "                print(f\"Retry request: {url_scrape} | Retry count: {retry+1} | Error: {e}\")\n",
    "                session = requests.Session()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    soup2 = soup.find(name=\"h2\", string=\"Jobs search results\").find_parent(\"div\")\n",
    "    total_jobs = soup2.select_one('div[role=\"status\"]').get('aria-label')\n",
    "\n",
    "    # write_json_file(\n",
    "    #     data=soup2,\n",
    "    #     url=url_scrape,\n",
    "    #     identifier=identifier,\n",
    "    #     scrape_date=scrape_date,\n",
    "    #     page=page,\n",
    "    #     total_jobs=total_jobs,\n",
    "    #     folder=\"../data/scraper\"\n",
    "    # )\n",
    "\n",
    "    data_export_json(\n",
    "        data=soup2,\n",
    "        website=website,\n",
    "        folder_name='scraper',\n",
    "        version=version,\n",
    "        content_date=scrape_date, # \"0000-00-00\"\n",
    "        additional_info=f\"scrape-page{page}\",\n",
    "        metadata={\n",
    "            \"total_jobs\": total_jobs,\n",
    "            \"page\": page,\n",
    "            \"url_scrape\": url_scrape\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_list(\n",
    "        soup: BeautifulSoup,\n",
    "        page: int\n",
    "    ):\n",
    "\n",
    "    items = soup.find_all(name=\"span\", string=\"Learn more\")\n",
    "    parsed_list = list()\n",
    "    base_url = \"https://www.google.com/about/careers/applications/\"\n",
    "\n",
    "    for item in items:\n",
    "        job_url = item.find_next(name=\"a\").get(\"href\")\n",
    "        job_url = f\"{base_url}{job_url}\"\n",
    "\n",
    "        print(f\"Parsing Page: {page} | Item: {len(parsed_list)+1} | URL: {job_url}\")\n",
    "        item2 = item.find_parent(name=\"li\")\n",
    "\n",
    "        job_id = item2.select_one(\"div\").attrs[\"jsdata\"].split(\";\")[1]\n",
    "        job_loc = item2.find(name=\"i\", string=\"place\").find_next(name=\"span\").text\n",
    "        job_name = item.find_parent(name=\"div\").find_parent(name=\"div\").find_parent(name=\"div\").find(name=\"h3\").text\n",
    "        try:\n",
    "            job_lvl = item2.find(name=\"i\", string=\"bar_chart\").find_next(name=\"span\").text\n",
    "        except (TypeError, AttributeError) as e:\n",
    "            print(e)\n",
    "            job_lvl = None\n",
    "\n",
    "        try:\n",
    "            job_lvl_desc = item2.find(name=\"i\", string=\"bar_chart\").find_next(name=\"span\").find_next(name=\"div\").find(name=\"h2\").find_next(\"div\").text\n",
    "        except (TypeError, AttributeError) as e:\n",
    "            print(e)\n",
    "            job_lvl_desc = None\n",
    "\n",
    "        job_qua = \" \".join([\n",
    "            f\"- {i.text}\" for i in item2.find(name=\"h4\", string=\"Minimum qualifications\").find_next(name=\"ul\").select(\"li\")\n",
    "        ])\n",
    "\n",
    "        data_dict = dict()\n",
    "        data_dict[\"id\"] = job_id\n",
    "        data_dict[\"url\"] = job_url\n",
    "        data_dict[\"name\"] = job_name\n",
    "        data_dict[\"location\"] = job_loc\n",
    "        data_dict[\"level\"] = job_lvl\n",
    "        data_dict[\"level_description\"] = job_lvl_desc\n",
    "        data_dict[\"qualification\"] = job_qua\n",
    "\n",
    "        # Append parsed list\n",
    "        parsed_list.append(data_dict)\n",
    "\n",
    "    return parsed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaafe3",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac959f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and Parse\n",
    "reparse_only = True\n",
    "continue_scraper = False\n",
    "session = requests.Session()\n",
    "parsed_list = list()\n",
    "page = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        page += 1\n",
    "\n",
    "        # Reparse only, no scrape | Continue scraper from latest page\n",
    "        if reparse_only or continue_scraper:\n",
    "            try:\n",
    "                json_data = data_import_json(\n",
    "                    website=website,\n",
    "                    folder_name='scraper',\n",
    "                    version=version,\n",
    "                    content_date=content_date,\n",
    "                    additional_info=f\"scrape-page{page}\"\n",
    "                )\n",
    "                soup = BeautifulSoup(\n",
    "                    json_data[\"data\"],\n",
    "                    \"html.parser\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                # Scrape\n",
    "                if reparse_only:\n",
    "                    print(e)\n",
    "                    break\n",
    "\n",
    "                # Continue Scraper\n",
    "                else:\n",
    "                    soup = scrape_job(\n",
    "                        session=session,\n",
    "                        page=page,\n",
    "                        scrape_date=content_date,\n",
    "                        website=website,\n",
    "                        version=version,\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            # Scrape\n",
    "            soup = scrape_job(\n",
    "                session=session,\n",
    "                page=page,\n",
    "                scrape_date=content_date,\n",
    "                website=website,\n",
    "                version=version,\n",
    "            )\n",
    "\n",
    "        # Parse\n",
    "        parsed_list_temp = parse_job_list(\n",
    "            soup=soup,\n",
    "            page=page\n",
    "        )\n",
    "\n",
    "        parsed_list = parsed_list + parsed_list_temp\n",
    "\n",
    "        # Identifier to stop iteration\n",
    "        if len(soup.select('a[aria-label=\"Go to next page\"]')) == 0:\n",
    "            break\n",
    "\n",
    "        # if page == 2:\n",
    "        #     break\n",
    "\n",
    "    df_parse = pd.DataFrame(parsed_list)\n",
    "    data_export_pandas(\n",
    "        df_output=df_parse,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name='parser',\n",
    "        additional_info='parsed',\n",
    "        incl_excel=True\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    df_parse = pd.DataFrame(parsed_list)\n",
    "    data_export_pandas(\n",
    "        df_output=df_parse,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name='parser',\n",
    "        additional_info='parsed',\n",
    "        incl_excel=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
