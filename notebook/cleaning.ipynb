{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c8f6c6",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5858191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_export_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9ee95",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af5bc1",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52668f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Gemini Dependencies\n",
    "from typing import List, Tuple, Optional, Any\n",
    "from google.genai import types, Client\n",
    "\n",
    "def _build_schema_from_defs(defs: Any) -> types.Schema:\n",
    "    if isinstance(defs, str):\n",
    "        # Primitive type\n",
    "        if defs == \"array_of_strings\" or defs == \"array\":\n",
    "            # Array of simple strings\n",
    "            return types.Schema(type=\"array\", items=types.Schema(type=\"string\"))\n",
    "        return types.Schema(type=defs)\n",
    "\n",
    "    if isinstance(defs, tuple):\n",
    "        if len(defs) == 2:\n",
    "            name, content = defs\n",
    "            # \"v\" tuple just unwraps\n",
    "            if name == \"v\":\n",
    "                return _build_schema_from_defs(content)\n",
    "            return types.Schema(type=\"object\", properties=_build_schema_from_defs(content))\n",
    "        elif len(defs) == 3:\n",
    "            # (name, children, \"array\") -> array of objects\n",
    "            name, children, marker = defs\n",
    "            if marker == \"array\":\n",
    "                return types.Schema(type=\"array\", items=_build_schema_from_defs(children))\n",
    "\n",
    "    if isinstance(defs, list):\n",
    "        props = {}\n",
    "        required = []\n",
    "        for part in defs:\n",
    "            if len(part) == 3 and part[2] == \"array\":\n",
    "                # Array of objects\n",
    "                name, sub, _ = part\n",
    "                props[name] = types.Schema(type=\"array\", items=_build_schema_from_defs(sub))\n",
    "            else:\n",
    "                name, sub = part\n",
    "                if sub == \"array_of_strings\" or sub == \"array\":\n",
    "                    props[name] = types.Schema(type=\"array\", items=types.Schema(type=\"string\"))\n",
    "                else:\n",
    "                    props[name] = _build_schema_from_defs(sub)\n",
    "            required.append(name)\n",
    "        return types.Schema(type=\"object\", properties=props, required=required)\n",
    "\n",
    "    raise ValueError(f\"Invalid schema definition: {defs}\")\n",
    "\n",
    "\n",
    "def _build_types_schema(field_defs: List[Tuple[str, Any]]) -> types.Schema:\n",
    "    \"\"\"\n",
    "    Build the top-level schema as an ARRAY of OBJECTS.\n",
    "\n",
    "    Each top-level tuple (name, sub) becomes a property on the item object.\n",
    "      - If sub is a list -> property = array(items = object defined by sub)\n",
    "      - If sub is a string/tuple -> property = schema returned by _build_schema_from_defs\n",
    "    \"\"\"\n",
    "    item_props = {}\n",
    "    item_required = []\n",
    "\n",
    "    for name, sub in field_defs:\n",
    "        if isinstance(sub, list):\n",
    "            item_schema = _build_schema_from_defs(sub)\n",
    "            item_props[name] = types.Schema(type=\"array\", items=item_schema)\n",
    "        else:\n",
    "            item_props[name] = _build_schema_from_defs(sub)\n",
    "\n",
    "        item_required.append(name)\n",
    "\n",
    "    return types.Schema(\n",
    "        type=\"array\",\n",
    "        items=types.Schema(\n",
    "            type=\"object\",\n",
    "            properties=item_props,\n",
    "            required=item_required\n",
    "        )\n",
    "    )\n",
    "\n",
    "def gemini_process_response(\n",
    "    model_version: str,\n",
    "    api_key: str,\n",
    "    prompt_template: str,\n",
    "    input: str,\n",
    "    column_uid: str,\n",
    "    response_key_list: Optional[List[Tuple[str, str]]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Request Gemini model and parse structured list of dicts\n",
    "    \"\"\"\n",
    "    # Ensure uid is always included\n",
    "    if response_key_list is None:\n",
    "        response_key_list = []\n",
    "    response_key_list = [(column_uid, \"string\")] + response_key_list\n",
    "\n",
    "    # Build schema from response_key_list\n",
    "    response_schema = _build_types_schema(response_key_list)\n",
    "\n",
    "    client = Client(api_key=api_key)\n",
    "    response = client.models.generate_content(\n",
    "        model=model_version,\n",
    "        contents=(\n",
    "            prompt_template +\n",
    "            \" Output must include uid exactly as provided in the input, without trimming, chopping, or normalization.\"\n",
    "            \" list input: \" + input\n",
    "        ),\n",
    "        config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_schema\": response_schema,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if response.candidates[0].finish_reason.name != 'STOP':\n",
    "        raise ValueError(\"response hit token output limit:\", response.usage_metadata.candidates_token_count)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6a356",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "website = ENV['SOURCE']['NAME']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "MODEL = ENV['LLM']['GEMINI']['MODEL']\n",
    "API_KEY = ENV['LLM']['GEMINI']['API_KEY']\n",
    "range_input = ENV['LLM']['GEMINI']['RANGE_INPUT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a data cleaning and normalization assistant.\n",
    "\n",
    "Your task is to take a messy text containing pricing information and convert it into a clean, structured JSON object. \n",
    "Make sure to:\n",
    "- Extract the minimum years of experience as integers (numbers only, without dots or commas).\n",
    "- Extract the minimum required degree.\n",
    "- Extract relevant experience role, e.g software development, ML infrastructure.\n",
    "- Fill empty value with empty string.\n",
    "\n",
    "Return output **strictly** in JSON with these fields:\n",
    "`uid`, `y` as `min_years`, `d` as `min_degree`, `e` as `relevant_experience`.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \n",
    "{{\n",
    "    \"uid\": \"1\",\n",
    "    \"qualification\": \"- Bachelorâ€™s degree or equivalent practical experience. - 2 years of experience with software development in one or more programming languages, or 1 year of experience with an advanced degree. - 1 year of experience with one or more of the following: speech/audio (e.g., technology duplicating and responding to the human voice), reinforcement learning (e.g., sequential decision making), or specialization in another ML field. - 1 year of experience with ML infrastructure (e.g., model deployment, model evaluation, optimization, data processing, debugging).\"\n",
    "}}\n",
    "\n",
    "Output:\n",
    "{{\n",
    "    \"uid\": \"1\",\n",
    "    \"y\": \"1\",\n",
    "    \"d\": \"bachelor\",\n",
    "    \"x\": \"software development, ML field\"\n",
    "}}\n",
    "\n",
    "Now clean the following input:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FIELD_DEFS = [\n",
    "    (\"r\", [\n",
    "        (\"y\", \"string\"),\n",
    "        (\"d\", \"string\"),\n",
    "        (\"x\", \"string\"),\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f261b",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = data_import_pandas(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name='parser',\n",
    "    additional_info='parsed',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Country\n",
    "def get_country(\n",
    "        df_input: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "    df_input['country'] = df_input['location'].str.split(',').str[-1].str.strip()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country(\n",
    "    df_input=df_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d882119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_preprocess_input(\n",
    "        df_input: pd.DataFrame,\n",
    "        content_date: datetime,\n",
    "        version: str\n",
    "    ):\n",
    "\n",
    "    # Assign content_date\n",
    "    df_input['content_date'] = str(content_date)\n",
    "\n",
    "    # Generate UID\n",
    "    df_input['uid'] = df_input.index + 1\n",
    "    df_input['uid'] = df_input['uid'].astype(str)\n",
    "    df_input['version'] = version\n",
    "\n",
    "    return\n",
    "\n",
    "def llm_generate_input(\n",
    "        df_input: pd.DataFrame,\n",
    "        process_column: list\n",
    "    ):\n",
    "\n",
    "    # Convert input to list\n",
    "    input_data = df_input[['uid'] + process_column].apply(dict, axis=1).to_list()\n",
    "\n",
    "    return input_data\n",
    "\n",
    "def llm_request_gemini(\n",
    "        prompt: str,\n",
    "        FIELD_DEFS: list,\n",
    "        input_data: list,\n",
    "        MODEL: str,\n",
    "        API_KEY: str\n",
    "    ):\n",
    "\n",
    "    response = gemini_process_response(\n",
    "        model_version=MODEL,\n",
    "        api_key=API_KEY,\n",
    "        prompt_template=prompt,\n",
    "        input=str(input_data),\n",
    "        column_uid='uid',\n",
    "        response_key_list=FIELD_DEFS\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_dump_response(\n",
    "        response: types.GenerateContentResponse,\n",
    "        content_date: datetime,\n",
    "        website: str,\n",
    "        version: str,\n",
    "        additional_info: str='response'\n",
    "    ):\n",
    "\n",
    "    # Convert to Python dict safely\n",
    "    response_dict = response.model_dump()\n",
    "\n",
    "    # Optionally save to file\n",
    "    data_export_json(\n",
    "        data=response_dict,\n",
    "        website=website,\n",
    "        folder_name='llm',\n",
    "        version=version,\n",
    "        content_date=content_date, # \"0000-00-00\"\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "\n",
    "def llm_merge_response(\n",
    "        df_input: pd.DataFrame,\n",
    "        response: types.GenerateContentResponse,\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: str,\n",
    "        folder_name: str,\n",
    "        additional_info: str='cleaning'\n",
    "    ):\n",
    "\n",
    "    # Get token usage\n",
    "    token_input = response.usage_metadata.prompt_token_count\n",
    "    token_output = response.usage_metadata.candidates_token_count\n",
    "    print(f\"{additional_info} | Token Input: {token_input} | Token Output: {token_output}\")\n",
    "\n",
    "    # Convert parsed response to dataframe\n",
    "    df_response = pd.DataFrame(response.parsed)\n",
    "    df_response.insert(0, \"token_input\", token_input)\n",
    "    df_response.insert(1, \"token_output\", token_output)\n",
    "\n",
    "    # # Dump converted response to json\n",
    "    # data_export_pandas(\n",
    "    #     df_output=df_response,\n",
    "    #     website=website,\n",
    "    #     content_date=content_date,\n",
    "    #     version=version,\n",
    "    #     folder_name='llm',\n",
    "    #     additional_info=additional_info, #'response-parsed'\n",
    "    #     # incl_excel=True\n",
    "    # )\n",
    "\n",
    "    # Merge response\n",
    "    df_input_merged = pd.merge(\n",
    "        left=df_input,\n",
    "        right=df_response,\n",
    "        on='uid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Export Merged Gemini Response with Input Data\n",
    "    data_export_pandas(\n",
    "        df_output=df_input_merged,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name=folder_name,\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "\n",
    "    return df_input_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_qualification(\n",
    "        df_input: pd.DataFrame,\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: str,\n",
    "        prompt: str,\n",
    "        FIELD_DEFS: list,\n",
    "        MODEL: str,\n",
    "        API_KEY: str,\n",
    "        process_column: list=['qualification'],\n",
    "        additional_info: str='cleaning',\n",
    "    ):\n",
    "\n",
    "    print(\"Preprocessing Input Data\")\n",
    "    try:\n",
    "        llm_preprocess_input(\n",
    "            df_input=df_input,\n",
    "            content_date=content_date,\n",
    "            version=version\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Generating Input List\")\n",
    "    try:\n",
    "        input_data = llm_generate_input(\n",
    "            df_input=df_input,\n",
    "            process_column=process_column\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Request Gemini Response\")\n",
    "    try:\n",
    "        response = llm_request_gemini(\n",
    "            prompt=prompt,\n",
    "            FIELD_DEFS=FIELD_DEFS,\n",
    "            input_data=input_data,\n",
    "            MODEL=MODEL,\n",
    "            API_KEY=API_KEY\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Dump Gemini Response to JSON\")\n",
    "    try:\n",
    "        llm_dump_response(\n",
    "            response=response,\n",
    "            content_date=content_date,\n",
    "            website=website,\n",
    "            version=version,\n",
    "            additional_info=f'response-{additional_info}'\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Merge Gemini Response to Input Data and Export\")\n",
    "    try:\n",
    "        df_input_merge = llm_merge_response(\n",
    "            df_input=df_input,\n",
    "            response=response,\n",
    "            website=website,\n",
    "            content_date=content_date,\n",
    "            version=version,\n",
    "            folder_name='cleaning',\n",
    "            additional_info=additional_info\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    return df_input_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import date_basic\n",
    "import sys\n",
    "sys.stdout = open(f\"../data/logging/{date_basic(content_date)}/{date_basic(content_date)}-cleaning.log\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "len_input = len(df_input)\n",
    "loop_count = ceil(len_input/range_input)\n",
    "\n",
    "borders = [(loop*range_input, (loop+1)*range_input) for loop in range(loop_count)][:-1]\n",
    "borders = borders + [((loop_count-1)*range_input, len_input)]\n",
    "\n",
    "df_merge = pd.DataFrame()\n",
    "\n",
    "for lower_border, upper_border in borders:\n",
    "\n",
    "    print(f\"Processing: {lower_border} - {upper_border}\")\n",
    "    df_input_temp = df_input.iloc[lower_border:upper_border].copy(deep=True)\n",
    "\n",
    "    df_merge_temp = cleaning_qualification(\n",
    "        df_input=df_input_temp,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        prompt=prompt,\n",
    "        FIELD_DEFS=FIELD_DEFS,\n",
    "        MODEL=MODEL,\n",
    "        API_KEY=API_KEY,\n",
    "        additional_info=f'cleaning-{lower_border}_{upper_border}'\n",
    "    )\n",
    "    print(f\"Completed: {lower_border} - {upper_border}\")\n",
    "\n",
    "    df_merge = pd.concat([\n",
    "        df_merge,\n",
    "        df_merge_temp\n",
    "    ])\n",
    "\n",
    "df_merge.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_merge,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name='cleaning',\n",
    "    additional_info='cleaning',\n",
    ")\n",
    "\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Remerge all results\n",
    "def concat_cleaning(\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: int,\n",
    "        borders: list[tuple]\n",
    "    ):\n",
    "\n",
    "    df_merge = pd.DataFrame()\n",
    "    for lower_border, upper_border in borders:\n",
    "        df_merge_temp = data_import_pandas(\n",
    "            website=website,\n",
    "            content_date=content_date,\n",
    "            version=version,\n",
    "            folder_name='cleaning',\n",
    "            additional_info=f'cleaning-{lower_border}_{upper_border}',\n",
    "        )\n",
    "\n",
    "        df_merge = pd.concat([\n",
    "            df_merge,\n",
    "            df_merge_temp\n",
    "        ])\n",
    "\n",
    "    df_merge.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_export_pandas(\n",
    "        df_output=df_merge,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name='cleaning',\n",
    "        additional_info='cleaning',\n",
    "    )\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = concat_cleaning(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    borders=borders\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853dfcb",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e755e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explode and mapping response\n",
    "def merge_mapping(\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: int\n",
    "    ):\n",
    "\n",
    "    df_merge = data_import_pandas(\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name='cleaning',\n",
    "        additional_info='cleaning',\n",
    "    )\n",
    "\n",
    "    df_merge = pd.concat([\n",
    "        df_merge,\n",
    "        pd.json_normalize(df_merge['r'].str[0])\n",
    "    ], axis=1)\n",
    "\n",
    "    df_merge.rename(columns={\n",
    "        \"r\": \"response_cleaning\",\n",
    "        \"y\": \"min_years\",\n",
    "        \"d\": \"min_degree\",\n",
    "        \"x\": \"related_experience\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    cleaning_cols = ['level_description', 'qualification', 'country', 'min_years', 'min_degree', 'related_experience']\n",
    "    for column in cleaning_cols:\n",
    "        df_merge[column] = df_merge[column].fillna('').astype(str).str.lower().str.strip().str.replace(f\"^(null|none|nan|na|n/a)$\", \"\", regex=True)\n",
    "\n",
    "    data_export_pandas(\n",
    "        df_output=df_merge,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name='cleaning',\n",
    "        additional_info='mapping',\n",
    "    )\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf631111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = merge_mapping(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
