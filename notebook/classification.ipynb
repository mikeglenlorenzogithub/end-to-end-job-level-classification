{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cef6f91",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0772259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_pkl, data_export_pkl, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179b99b",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63759bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "website = ENV['SOURCE']['NAME']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "grid_search = ENV['CLASSIFICATION']['GRID_SEARCH']\n",
    "load_pipeline = ENV['CLASSIFICATION']['LOAD_PIPELINE']\n",
    "is_weight = ENV['CLASSIFICATION']['IS_WEIGHT']\n",
    "is_pca = ENV['CLASSIFICATION']['IS_PCA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c945a9a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafac39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = data_import_pandas(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name='embeddings',\n",
    "    additional_info='embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764e2a8",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebae66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "# 3. Load saved embeddings\n",
    "X_qual_embed = df_embed['qualification_embedding'].to_list()\n",
    "X_qual_embed = np.array(X_qual_embed)\n",
    "\n",
    "# 4 TF-IDF for related_experience\n",
    "df_embed['related_experience'] = df_embed['related_experience'].fillna('')\n",
    "tfidf_exp = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "X_exp = tfidf_exp.fit_transform(df_embed['related_experience'])\n",
    "\n",
    "# 5. Numeric features\n",
    "## Encode categorical\n",
    "degree_map = {\n",
    "    '': 0,\n",
    "    '0': 0,\n",
    "    'unspecified': 0,\n",
    "\n",
    "    'high school diploma': 1,\n",
    "    'mbo': 1,\n",
    "    'hbo': 1,\n",
    "\n",
    "    'associate': 2,\n",
    "    'associates': 2,\n",
    "\n",
    "    'bachelor': 3,\n",
    "    'bachelors': 3,\n",
    "    'ba/bs': 3,\n",
    "\n",
    "    'master': 4,\n",
    "    'mba': 4,\n",
    "\n",
    "    'phd': 5,\n",
    "    'doctoral': 5,\n",
    "    'doctor': 5,      # Sometimes appears as \"doctor\"\n",
    "    'jd': 5,\n",
    "\n",
    "    # Domain-specific advanced license â†’ treat as postgraduate level\n",
    "    'faa airline transport pilot certificate': 5\n",
    "}\n",
    "\n",
    "df_embed['min_years'] = df_embed['min_years'].fillna(0).replace('', 0).astype(int)\n",
    "df_embed['min_degree'] = df_embed['min_degree'].map(degree_map).fillna(0).astype(int)\n",
    "df_embed['country_encoded'] = LabelEncoder().fit_transform(df_embed['country'])\n",
    "\n",
    "X_numeric = df_embed[['min_years', 'min_degree', 'country_encoded']].fillna(0).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_embed['level_encoded'] = le.fit_transform(df_embed['level'])\n",
    "\n",
    "# Target\n",
    "y = df_embed['level_encoded']  # e.g., 'Entry', 'Mid', 'Senior'\n",
    "y_level_count = df_embed['level'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95422cab",
   "metadata": {},
   "source": [
    "### Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_weight:\n",
    "    # Handle Weights due to Imbalance Class\n",
    "    level_counts = df_embed['level'].value_counts()\n",
    "\n",
    "    # Compute weights\n",
    "    total_samples = len(df_embed)\n",
    "    class_weights = {level: total_samples/count for level, count in level_counts.items()}\n",
    "    print(class_weights)\n",
    "\n",
    "    # Map weights to each sample\n",
    "    sample_weights = df_embed['level'].map(class_weights)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea136b0",
   "metadata": {},
   "source": [
    "### Load Saved Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pipeline:\n",
    "    # Load the saved pipeline\n",
    "    pipeline_objects = data_import_pkl(\n",
    "        website=website,\n",
    "        folder_name='classification',\n",
    "        version=version,\n",
    "        content_date=content_date,\n",
    "        additional_info='pipeline-job_level'\n",
    "    )\n",
    "\n",
    "    # Extract objects\n",
    "    model = pipeline_objects['model']\n",
    "    tfidf_exp = pipeline_objects['tfidf_exp']\n",
    "    X_qual_embed = pipeline_objects['embeddings_qual']\n",
    "    X_exp = pipeline_objects['X_exp']\n",
    "    X_numeric = pipeline_objects['X_numeric']\n",
    "    y_level_count = pipeline_objects['y_level_count']\n",
    "    y = pipeline_objects['y_level_encoded']\n",
    "    le = pipeline_objects['label_encoder_level']\n",
    "    best_params_ = pipeline_objects['best_params_']\n",
    "\n",
    "    print(\"Pipeline and model loaded successfully!\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f82724",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321742f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_pca:\n",
    "    # Apply PCA to reduce tf-idf dimensionality\n",
    "    svd_exp = TruncatedSVD(n_components=50, random_state=42)\n",
    "    X_exp_reduced = svd_exp.fit_transform(X_exp)\n",
    "\n",
    "    # Apply PCA to reduce embeddings dimensionality\n",
    "    pca_dim = min(X_qual_embed.shape[0], X_qual_embed.shape[1], 100)  # automatically safe reduce 1536-dim embedding to 100 if sample less then min\n",
    "    pca_qual = PCA(n_components=pca_dim, random_state=42)\n",
    "    pca_qual.fit(X_qual_embed)  # fit on source embeddings\n",
    "\n",
    "    X_qual_embed_reduced = pca_qual.transform(X_qual_embed)\n",
    "\n",
    "else:\n",
    "    X_exp_reduced = X_exp\n",
    "    X_qual_embed_reduced = X_qual_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032e3a1",
   "metadata": {},
   "source": [
    "### Sparse and Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Embeddings and Numeric to Compressed Sparse Row due to TF-IDF\n",
    "# Convert to sparse to stack with TF-IDF\n",
    "X_qual_embed_sparse = csr_matrix(X_qual_embed_reduced)\n",
    "\n",
    "# Convert numeric to sparse to stack with TF-IDF\n",
    "X_numeric_sparse = csr_matrix(X_numeric)\n",
    "\n",
    "# Combine features\n",
    "X = hstack([X_exp_reduced, X_qual_embed_sparse, X_numeric_sparse])\n",
    "\n",
    "# Split Train Test (optional weight)\n",
    "if is_weight:\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c92648",
   "metadata": {},
   "source": [
    "### Grid Search to get the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf09c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search looking for the best hyperparameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if grid_search and not load_pipeline:\n",
    "    param_grid = {\n",
    "        'max_depth': [4,6,8],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1_weighted',\n",
    "        cv=5,\n",
    "        verbose=3,\n",
    "        n_jobs=-2\n",
    "    )\n",
    "\n",
    "    if is_weight:\n",
    "        grid_search.fit(X_train, y_train, sample_weight=w_train)\n",
    "    else:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    print(\"Best params:\", grid_search.best_params_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "### Direct Train XGBoost\n",
    "else:\n",
    "    if load_pipeline:\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=best_params_['n_estimators'],\n",
    "            max_depth=best_params_['max_depth'],\n",
    "            learning_rate=best_params_['learning_rate'],\n",
    "            subsample=best_params_['subsample'],\n",
    "            colsample_bytree=best_params_['colsample_bytree'],\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if is_weight:\n",
    "        model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486a8f0",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ffd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Direct\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().apply(lambda value: round(value, 2))\n",
    "\n",
    "additional_info = 'performance-job_level'\n",
    "if is_pca:\n",
    "    additional_info=additional_info+'-pca'\n",
    "if is_weight:\n",
    "    additional_info=additional_info+'-weight'\n",
    "if grid_search:\n",
    "    additional_info=additional_info+'-grid_search'\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_report,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name='classification',\n",
    "    additional_info=additional_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa315410",
   "metadata": {},
   "source": [
    "### Feed Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Overall Data\n",
    "y_pred_all = best_model.predict(X)\n",
    "df_embed['predicted_level_encoded'] = y_pred_all\n",
    "df_embed['predicted_level'] = le.inverse_transform(y_pred_all)\n",
    "data_export_pandas(\n",
    "    df_output=df_embed,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name='classification',\n",
    "    additional_info='classification',\n",
    "    incl_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483d7d0",
   "metadata": {},
   "source": [
    "### Export pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11093fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assume features are already computed\n",
    "# X_exp_reduced : TF-IDF + SVD\n",
    "# X_qual_embed : raw embeddings\n",
    "# X_qual_embed_reduced : PCA-reduced embeddings\n",
    "# X_numeric : numeric features\n",
    "# X : combined sparse features\n",
    "# model : trained XGBoost\n",
    "# y_test, y_pred : for evaluation\n",
    "\n",
    "# Save pipeline + features\n",
    "pipeline_objects = {\n",
    "    'model': model,\n",
    "    'tfidf_exp': tfidf_exp,\n",
    "    'embeddings_qual': X_qual_embed,     # PCA-reduced embeddings\n",
    "    'X_exp': X_exp,                      # TF-IDF + SVD\n",
    "    'X_numeric': X_numeric,                              # numeric features\n",
    "    'y_level_count': y_level_count,\n",
    "    'y_level_encoded': y,\n",
    "    'label_encoder_level': LabelEncoder().fit(df_embed['level']),  # for decoding\n",
    "    'best_params_': grid_search.best_params_\n",
    "}\n",
    "\n",
    "data_export_pkl(\n",
    "    pipeline_objects=pipeline_objects,\n",
    "        website=website,\n",
    "        folder_name='classification',\n",
    "        version=version,\n",
    "        content_date=content_date,\n",
    "        additional_info='pipeline-job_level'\n",
    "    )\n",
    "print(\"All features, embeddings, preprocessing, and model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281552c0",
   "metadata": {},
   "source": [
    "## Visualize Performance-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(df_embed['level_encoded'], df_embed['predicted_level_encoded'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=le.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(df_embed['level_encoded'], df_embed['predicted_level_encoded'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=le.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "report = classification_report(df_embed['level_encoded'], \n",
    "                               df_embed['predicted_level_encoded'], \n",
    "                               target_names=le.classes_,\n",
    "                               output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "report_df[['precision', 'recall', 'f1-score']].iloc[:-3].plot(kind='bar')\n",
    "plt.title(\"Performance Metrics per Job Level\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292a24b",
   "metadata": {},
   "source": [
    "# Visualize Performance Data / prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Predicted vs Actual\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='level', hue='predicted_level', data=df_embed)\n",
    "plt.title(\"Actual vs Predicted Job Levels\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Feature importance\n",
    "max_num_features = 10\n",
    "\n",
    "# Sizes of each feature block\n",
    "n_tfidf = X_exp_reduced.shape[1]           # e.g., 50\n",
    "n_embed = X_qual_embed_reduced.shape[1]    # e.g., 100\n",
    "n_numeric = X_numeric.shape[1]             # e.g., 3\n",
    "\n",
    "# TF-IDF reduced features\n",
    "tfidf_names = [f\"tfidf_{i}\" for i in range(n_tfidf)]\n",
    "\n",
    "# Embedding PCA features\n",
    "embed_names = [f\"embed_pca_{i}\" for i in range(n_embed)]\n",
    "\n",
    "# Numeric features (use your original numeric column names)\n",
    "numeric_names = ['min_years', 'min_degree', 'country_encoded']\n",
    "\n",
    "# Combine all names in the same order as X\n",
    "feature_names = tfidf_names + embed_names + numeric_names\n",
    "\n",
    "xgb.plot_importance(best_model, max_num_features=max_num_features, importance_type='weight', \n",
    "                    xlabel='F Score', grid=True)\n",
    "plt.show()\n",
    "\n",
    "importance = model.get_booster().get_score(importance_type='weight')\n",
    "# Map f0..fn to actual names\n",
    "importance_named = {feature_names[int(k[1:])]: v for k, v in importance.items()}\n",
    "importance_df = pd.DataFrame.from_dict(importance_named, orient='index', columns=['fscore'])\n",
    "importance_df = importance_df.sort_values(by='fscore', ascending=False)\n",
    "print(importance_df.head(max_num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Embedding visualization (optional, for portfolio)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_emb_2d = tsne.fit_transform(X_qual_embed_reduced)\n",
    "\n",
    "df_embed['emb_2d_x'] = X_emb_2d[:,0]\n",
    "df_embed['emb_2d_y'] = X_emb_2d[:,1]\n",
    "\n",
    "sns.scatterplot(x='emb_2d_x', y='emb_2d_y', hue='level', data=df_embed, palette='tab10')\n",
    "plt.title(\"Qualification Embeddings (2D)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
